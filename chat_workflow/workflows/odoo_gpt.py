from ..llm import llm_factory, ModelCapability
from ..tools import BasicToolNode
from chainlit.input_widget import Select
from chat_workflow.workflows.base import BaseWorkflow, BaseState
from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, MessagesPlaceholder
from langchain_core.runnables import Runnable, RunnableConfig
from langgraph.graph import StateGraph, END
from typing import List, Literal
from typing_extensions import TypedDict
import chainlit as cl

class BAResponse(TypedDict):
    """Structured response for ba_node."""
    messages: str  
    requirements: str  # Updated requirements for the project


class CodeFile(TypedDict):
    """Represents a single code file."""
    filename: str  # The name of the file
    location: str  # The folder where the file is located
    content: str   # The code content


class CoderResponse(TypedDict):
    """Structured response for coder_node."""
    files: List[CodeFile]  # List of files generated by the coder
    messages: str  # Status or notes about the code generation


class Router(TypedDict):
    """Worker to route to next. If no workers needed, route to __end__"""
    next: Literal["business_analyst", "coder", "responder"]
    messages: str      


# Define the State Class
class GraphState(BaseState):
    chat_model: str  # Model used by the chatbot
    requirements: str  # User-provided requirements
    code_files: List[CodeFile]  # Output files from the Coder Node
    next: str


# Define the Workflow
class OdooGPTWorkflow(BaseWorkflow):
    def __init__(self):
        super().__init__()
        self.capabilities = {ModelCapability.TEXT_TO_TEXT, ModelCapability.TOOL_CALLING}
        self.tools = []

    @classmethod
    def name(self) -> str:
        return "Odoo-GPT"

    @property
    def output_chat_model(self) -> str:
        return "gpt-4o-mini"  # Example: Use GPT-4 for final responses

    @classmethod
    def chat_profile(cls):
        return cl.ChatProfile(
            name=cls.name(),
            markdown_description="An assistant that helps build Odoo custom modules.",
            icon="https://cdn2.iconfinder.com/data/icons/3d-resume/128/5_Experience.png",
            starters=[
                cl.Starter(
                    label="Add VIP field",
                    message="Add field VIP, default yes to res.partner module",
                    icon="https://cdn0.iconfinder.com/data/icons/3d-graphic-design-tools-1/128/Zoom_In.png",
                ),
            ],
        )

    @property
    def chat_settings(self) -> cl.ChatSettings:
        return cl.ChatSettings([
            Select(
                id="chat_model",
                label="Chat Model",
                values=sorted(llm_factory.list_models(
                    capabilities=self.capabilities)),
                initial_index=-1,
            )
        ])

    def create_default_state(self) -> GraphState:
        # Initialize the default state variables
        return {
            "name": self.name(),
            "messages": [],
            "chat_model": "gpt-4o-mini",
            "requirements": "",
            "code_files": [],
            "next": "responder",
        }

    def create_graph(self) -> StateGraph:
        # Create the state graph
        graph = StateGraph(GraphState)

        # Add nodes (agents)
        graph.add_node("supervisor", self.supervisor_node)
        graph.add_node("tools", BasicToolNode(self.tools))
        graph.add_node("business_analyst", self.ba_node)
        graph.add_node("coder", self.coder_node)
        graph.add_node("responder", self.responder_node)
        # Add edges (transitions)
        graph.set_entry_point("supervisor")
        graph.add_conditional_edges("supervisor", lambda state: state["next"])
        graph.add_edge("business_analyst", "supervisor")
        graph.add_edge("coder", "supervisor")
        graph.add_edge("tools", "supervisor")
        graph.add_edge("responder", END)

        return graph

    ### Define Node Methods ###
    async def supervisor_node(self, state: GraphState, config: RunnableConfig) -> GraphState:
        print("Running: supervisor_node")
        system_prompt = SystemMessagePromptTemplate.from_template("""
You are a supervisor coordinating tasks between the business_analyst, coder, and responder agents to develop a custom Odoo 17 module.  
Based on the current state:
- Assign the next agent (`business_analyst`, `coder`, or `responder`) to perform a task.
- Include any instructions or context for the agent in your response.
- If user input is required or the task is complete, assign the responder agent to communicate with the user.

Current progress: {requirements}
        """)
        prompt = ChatPromptTemplate.from_messages([
            system_prompt,
            MessagesPlaceholder(variable_name="messages"),
        ])

        llm = llm_factory.create_model(
            self.output_chat_model,
            model=state["chat_model"],
            tools=self.tools
        ).with_structured_output(Router)

        chain: Runnable = prompt | llm
        response = await chain.ainvoke(state, config=config)
        print(f"supervisor_node response: {response}")
        state.update({"messages": [{"role": "ai", "content": response["messages"]}], "next": response["next"]})
        return state

    async def ba_node(self, state: GraphState, config: RunnableConfig) -> GraphState:
        print("Running: ba_node")
        system_prompt = SystemMessagePromptTemplate.from_template("""
You are a business analyst agent supervised by an AI supervisor.  
Your task is to:
1. Clarify and refine the current requirements for the Odoo engineer.
2. Provide the updated requirements.
3. Include a brief status message for the supervisor about any additional actions or clarifications needed.

Current requirements: {requirements}
        """)
        prompt = ChatPromptTemplate.from_messages([
            system_prompt,
            MessagesPlaceholder(variable_name="messages"),
        ])

        llm = llm_factory.create_model(
            self.output_chat_model,
            model=state["chat_model"]
        ).with_structured_output(BAResponse)
        chain: Runnable = prompt | llm

        response = await chain.ainvoke({
            "messages": state["messages"], 
            "requirements": state["requirements"]}, config=config)
        print(f"ba_node response: {response}")

        state.update({
            "requirements": response['requirements'], 
            "messages": [{"role": "assistant", "content": response['messages']}]
        })
        return state

    async def coder_node(self, state: GraphState, config: RunnableConfig) -> GraphState:
        print("Running: coder_node")
        system_prompt = SystemMessagePromptTemplate.from_template("""
You are a coder agent supervised by an AI supervisor.  
Your task is to:
1. Generate code files for the Odoo 17 module based on the provided requirements.
2. You must include a brief status message for the supervisor with any issues encountered.

Requirements: {requirements}
Codes: {code_files}
        """)
        prompt = ChatPromptTemplate.from_messages([
            system_prompt,
            MessagesPlaceholder(variable_name="messages"),
        ])

        llm = llm_factory.create_model(
            self.output_chat_model,
            model=state["chat_model"]
        ).with_structured_output(CoderResponse)
        chain: Runnable = prompt | llm

        response = await chain.ainvoke({
            "messages": state["messages"], 
            "requirements": state["requirements"], 
            "code_files": state["code_files"]}, config=config)
        print(f"coder_node response: {response}")

        state.update({
            "code_files": response['files'],
            "messages": [{"role": "assistant", "content": response['messages']}]
        })
        return state

    async def responder_node(self, state: GraphState, config: RunnableConfig) -> GraphState:
        print("Running: responder_node")
        system_prompt = SystemMessagePromptTemplate.from_template("""
You are a helpful AI Assistant that will converse with the user to help them develop a custom module. 
You are supervised by an AI Agent.
- Current requirements provided by our Business Analyst agent: {requirements}.
- Code files generated by the Coder agent (if available): {code_files}.

If requirements and code are clear enough, show them to the user when responding.
        """)
        prompt = ChatPromptTemplate.from_messages([
            system_prompt,
            MessagesPlaceholder(variable_name="messages"),
        ])

        llm = llm_factory.create_model(
            self.output_chat_model,
            model=state["chat_model"]
        )
        chain: Runnable = prompt | llm

        response = await chain.ainvoke({
            "messages": state["messages"], 
            "requirements": state["requirements"], 
            "code_files": state["code_files"]}, config=config)
        print(f"responder_node response: {response}")

        state.update({"messages": [response]})
        return state
